{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not functools.partial",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-562fac412472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mderivative_estimate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifference_quotient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"derivative_estimate : %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mderivative_estimate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# 畫出圖形可看出兩者基本上是相同的\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not functools.partial"
     ]
    }
   ],
   "source": [
    "'''\n",
    "梯度遞減 (gradient descent)\n",
    "'''\n",
    "# 一個實數向量做為輸入，輸出一個單一的實數\n",
    "# 讓函數得出最大 (或最小)的可能值\n",
    "from functools import partial\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    '''\n",
    "    計算 v 之中所有元素的平方和\n",
    "    '''\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "# 梯度的估算\n",
    "'''\n",
    "點 x 的導數 (derivative)\n",
    "    衡量 x 出現小小變化時 f(x)跟著變化的程度\n",
    "        差商 (difference quotient)\n",
    "'''\n",
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h     # h 趨近於 0\n",
    "\n",
    "# 平方函數\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "# 平方函數的導數\n",
    "def derivative(x):\n",
    "    return 2 * x\n",
    "\n",
    "derivative_estimate = partial(difference_quotient, square, h=0.00001)\n",
    "print(\"derivative_estimate : %.4f\" % derivative_estimate)\n",
    "\n",
    "# 畫出圖形可看出兩者基本上是相同的\n",
    "import matplotlib.pyplot as plt\n",
    "x = list(range(-10, 10))\n",
    "plt.title(\"Actual Dervatives vs. Estimates\")\n",
    "plt.plot(x, map(derivative, x), 'rx', label='Actual')        # rx 紅色的 x\n",
    "plt.plot(x, map(derivative_estimate, x), 'b+', label='Estimate')      # b+ 藍色的 +\n",
    "\n",
    "plt.lengend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'distance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-05df0dbc72c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 計算偏導數\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlinear_algebra\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_subtract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalar_multiply\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpartial_difference_quotient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'distance'"
     ]
    }
   ],
   "source": [
    "# 計算偏導數\n",
    "from linear_algebra import distance, vector_subtract, scalar_multiply\n",
    "import random\n",
    "\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    '''\n",
    "    計算 f 在 v 中第 i 個元素對應的差商\n",
    "    '''\n",
    "    w = [v_j + (h if j == i else 0)      # 只針對 v 的第 i 個元素，加上 h 的變動量\n",
    "        for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "# 同樣來估計梯度\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "           for i, _ in enumerate(v)]\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    '''\n",
    "    從 v 沿著 direction 的方向移動 step_size 的距離\n",
    "    '''\n",
    "    return [v_i + step_size * direction_i\n",
    "           for v_i, direction_i, in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# 取個隨機起始點\n",
    "v = [random.randint(-10, 10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v)       # 計算出 v 所對應的梯度\n",
    "    next_v = step(v, gradient, -0.01)           # 往梯度的負方向跨一小步\n",
    "    if distance(next_v, v) < tolerance:         # 結果收斂到一定程度，就停止\n",
    "        break\n",
    "    v = next_v                                  # 若未收斂，則繼續\n",
    "\n",
    "    \n",
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "# 使用例外處理\n",
    "def safe(f):\n",
    "    '''\n",
    "    送回來的值與原函數 f 相同，\n",
    "    但如果 f 出現錯誤，就送回一個無限大的結果\n",
    "    '''\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')             # 在 python中表示「無限大」\n",
    "        return safe_f\n",
    "\n",
    "print(\"safe :\", safe(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-16-35fc89da0ced>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-35fc89da0ced>\"\u001b[1;36m, line \u001b[1;32m23\u001b[0m\n\u001b[1;33m    return theta\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# 全部整合起來\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    '''\n",
    "    運用梯度遞減的做法，找出能讓目標函數值最小畫的相應 theta 值\n",
    "    '''\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0                   # 為 theta 設定初始值\n",
    "    target_fn = safe(target_fn)       # 目標函數 target_fn 的安全版\n",
    "    value = target_fn(theta)          # 想要的最小化值\n",
    "    \n",
    "while True:\n",
    "    gradient = gradient_fn(theta)\n",
    "    next_thetas = [step(theta, gradient, -step_size)\n",
    "                  for step_size in step_sizes]\n",
    "    \n",
    "    # 選出能讓誤差函數值最小化的相應值\n",
    "    next_theta = min(next_thetas, key=target_fn)\n",
    "    next_value = target_fn(next_theta)\n",
    "    \n",
    "    # 若結果收斂到一定程度，就停止下來\n",
    "    if abs(value - next_value) < tolerance:\n",
    "        return theta\n",
    "    else:\n",
    "        theta, value = next_theta, next_value\n",
    "        \n",
    "def negate(f):\n",
    "    '''\n",
    "    針對函數每個輸入值 x，全都送回相應的負函數值 -f(x)\n",
    "    '''\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    '''\n",
    "    如果 f 送回一串數字列表，也會一樣\n",
    "    '''\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                         negate_all(gradient_fn),\n",
    "                         theta_0,\n",
    "                         tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機梯度遞減\n",
    "import random\n",
    "\n",
    "def in_random_order(data):\n",
    "    '''\n",
    "    生成器會以隨機的順序送回整組資料中的每個元素\n",
    "    '''\n",
    "    indexes = [i for i, _ in enumerate(data)]       # 建立索引列表\n",
    "    random.shuffle(indexes)    # 打亂順序\n",
    "    for i in indexes:          # 按照新的順序送回資料\n",
    "        yield data[i]\n",
    "        \n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0 = 0.01):\n",
    "    data = zip(x, y)\n",
    "    theta = theta_0\n",
    "    alpha = alpha_0\n",
    "    min_theta, min_value = None, float(\"inf\")\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # 如果進行 100 次的迭代卻沒有任何改進，就停止\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( taaget_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            # 如果找到一個新的最小值，就把它紀錄下來\n",
    "            # 並且恢復到原始的間隔長度\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # 若沒有改善，就縮小間隔長度\n",
    "            interations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # 針對每個資料點，進行一次沿梯度跨步的動作\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "        return min_theta\n",
    "    \n",
    "# print(\"minimize_stochastic :\", minimize_stochastic(0.852, 0.556, x, y, theta_0, alpha_0))\n",
    "\n",
    "\n",
    "# 求取最大值\n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0 = 0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                              negate_all(gradient_fn),\n",
    "                              x, y, theta_0, alpha_0)\n",
    "\n",
    "# print(\"maximize_stochastic :\", maximize_stochastic(0.852, 0.556, x, y, theta_0, alpha_0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
