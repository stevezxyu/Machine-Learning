{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22acc5e0198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "自然語言處理 (Natural Language Processing, NLP)\n",
    "'''\n",
    "import math, random, re\n",
    "from collections import defaultdict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "\n",
    "def plot_resumes(plt):\n",
    "    data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),\n",
    "         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),\n",
    "         (\"data science\", 60, 70), (\"analytics\", 90, 3),\n",
    "         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),\n",
    "         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),\n",
    "         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),\n",
    "         (\"thought leadership\", 35, 35)]\n",
    "    \n",
    "    def text_size(total):\n",
    "        '''\n",
    "        如果 total 是 0 就送出 8，如果 total 是 200 就送出 28\n",
    "        '''\n",
    "        return 8 + total / 200 * 20\n",
    "    \n",
    "    for word, job_popularity, resume_popularity in data:\n",
    "        plt.text(job_popularity, resume_popularity, word,\n",
    "                ha='center', va='center',\n",
    "                size=text_size(job_popularity + resume_popularity))\n",
    "        \n",
    "plt.xlabel(\"Popularity on Job Postings\")\n",
    "plt.ylabel(\"Popularity on Resumes\")\n",
    "plt.axis([0, 100, 0, 100])\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n-gram 模型\n",
    "'''\n",
    "# 消除文字中的撇號\n",
    "def fix_unicode(text):\n",
    "    return text.replace(u\"\\u2019\", \"'\")\n",
    "\n",
    "# 把文字拆成一系列的單詞和句號\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_document():\n",
    "    url = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "    content = soup.find(\"div\", \"article-body\")     # 在 div 的內容中尋找\n",
    "    regex = r\"[\\w']+|[\\.]\"                          # 單詞或句號\n",
    "\n",
    "    document = []\n",
    "\n",
    "    for paragraph in content(\"p\"):\n",
    "        words = re.fillall(regex, fix_unicode(paragraph.text))\n",
    "        document.extend(words)\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 zip 壓合和連接兩個單詞\n",
    "# 2-gram model\n",
    "\n",
    "bigrams = zip(document, document[1:])\n",
    "transitions = defaultdict(list)\n",
    "for prev, current in bigrams:\n",
    "    transition[prev].append(current)\n",
    "    \n",
    "def generate_using_bigrams(transitions):\n",
    "    current = \".\"   # 這表示下一個單詞將是某個句子的起始單詞\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]    # bigrams (current, _)\n",
    "        current = random.choice(next_word_candidates)  # 隨機挑選其中一個\n",
    "        result.append(current)                         # 添加到結果後面\n",
    "        if current == \".\": return \" \".join(result)     # 如果遇到 \".\" 就完成了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-gram model\n",
    "\n",
    "trigrams = zip(document, document[1:], document[2:])\n",
    "trigram_transitions = defaultdict(list)\n",
    "starts = []\n",
    "\n",
    "for prev, current, next in trigrams:\n",
    "    if prev == \".\":                 # 如果前一個單詞是句號\n",
    "        starts.append(current)      # 目前這個就是起始單詞\n",
    "        \n",
    "    trigram_transitions[(prev, current)].append(next)\n",
    "\n",
    "    \n",
    "def generate_using_trigrams(starts, trigram_transitions):\n",
    "    current = random.choice(starts)   # 隨機選擇一個初始單辭\n",
    "    prev = \".\"                        # 前面是一個句號\n",
    "    result = [current]\n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next = random.choice(next_word_candidates)\n",
    "\n",
    "        prev, current = current, next\n",
    "        result.append(current)\n",
    "\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "文法 (grammar)\n",
    "'''\n",
    "grammar = {\n",
    "        \"_S\"  : [\"_NP _VP\"],\n",
    "        \"_NP\" : [\"_N\",\n",
    "                 \"_A _NP _P _A _N\"],\n",
    "        \"_VP\" : [\"_V\",\n",
    "                 \"_V _NP\"],\n",
    "        \"_N\"  : [\"data science\", \"Python\", \"regression\"],\n",
    "        \"_A\"  : [\"big\", \"linear\", \"logistic\"],\n",
    "        \"_P\"  : [\"about\", \"near\"],\n",
    "        \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "    }\n",
    "\n",
    "# 把 token 列表轉換成一個句子\n",
    "def is_terminal(token):\n",
    "    return tocken[0] != \"_\"\n",
    "\n",
    "# 以空白隔開的非終端文字來拆散\n",
    "def expand(grammar, tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 跳過終端文字\n",
    "        if is_terminal(token): continue\n",
    "        \n",
    "        # 如果遇上一個非終端文字\n",
    "        # 必須隨機選一個來取代\n",
    "        replacement = random.choice(grammar[token])\n",
    "        \n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "        \n",
    "        # 現在再針對新的 token 列表，遞迴呼叫 expand 函式\n",
    "        return expand(grammar, tokens)\n",
    "    \n",
    "    # 全部都是終端文字時，就完成\n",
    "    return tokens\n",
    "\n",
    "# 製造句子\n",
    "def generate_sentence(grammar):\n",
    "    return expand(grammar, [\"_S\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gibbs 取樣\n",
    "'''\n",
    "import random\n",
    "\n",
    "random.random()\n",
    "# inverse_normal_cdf(random.random())\n",
    "\n",
    "# 範例: 擲骰子\n",
    "def roll_a_die():\n",
    "    return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "# 在已知 x的條件下，y 的分布\n",
    "def random_y_given_x(x):\n",
    "    '''\n",
    "    只可能等於 x + 1, x + 2, ... , x + 6 \n",
    "    '''\n",
    "    return x + roll_a_die()\n",
    "\n",
    "# 在已知 y的條件下，x 的分布\n",
    "def random_x_given_y(y):\n",
    "    if y <= 7:\n",
    "        # 如果總點數小於等於 7，第一個骰子的點數就有可能是\n",
    "        # 1, 2, ..., (總點數 - 1)\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        # 如果總點數大於 7，第一個骰子就有可能是\n",
    "        # (總點數 - 6), (總點數 - 5), ..., 6\n",
    "        return random.randrange(y - 6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 經過重複多次迭代\n",
    "# 無條件聯合分布的一個樣本值\n",
    "def gibbs_sample(num_iters=100):\n",
    "    x, y = 1, 2          # 只要合理，選甚麼值都沒關係\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples=1000):\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_topic_counts[3][1] : 0\n",
      "document_topic_counts[2]['nlp'] : 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "主題模型話 (topic model)\n",
    "'''\n",
    "def sample_from(weights):\n",
    "    '''\n",
    "    根據 (權重值 / 權重總和) 的機率，進行隨機取樣，然後送出權重索引值 i\n",
    "    '''\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()       # 0 到 total 之間，每個值出現的機率皆相同\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                        # 送回滿足以下條件的最小 i 值\n",
    "        if rnd <= 0: return i           # sum(weights[:(i+1)]) >= rnd\n",
    "        \n",
    "        \n",
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]\n",
    "\n",
    "K = 4\n",
    "\n",
    "# 計數結果列表，每個文件都對應一組主題計數結果\n",
    "document_topic_counts = [Counter()\n",
    "                         for _ in documents]\n",
    "\n",
    "# 計數結果列表，每個主題對應一組單詞計數結果\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# 數字列表，每個主題對應一個數字\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# 數字列表，每個文件對應一個數字\n",
    "document_lengths = [len(d) for d in documents]\n",
    "\n",
    "# 不重複的單詞數量 (W)\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "# 文件的數量 (D)\n",
    "D = len(documents)\n",
    "\n",
    "# document[3]中被歸類為主題 1 的單詞數量\n",
    "print(\"document_topic_counts[3][1] :\", document_topic_counts[3][1])\n",
    "# 主題 2中 nlp 這個單詞出現的次數\n",
    "print(\"document_topic_counts[2]['nlp'] :\", document_topic_counts[2]['nlp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定義條件機率函數\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    '''\n",
    "    在文件 d 中，被指定為某主題的單詞所佔比例\n",
    "    也就是某主題出現在文件 d 中的機率 (已導入平滑項)\n",
    "    '''\n",
    "    return ((document_topic_counts[d][topic] + alpha) / \n",
    "            (document_lengths[d] + K * alpha))\n",
    "            \n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    '''\n",
    "    被指定為某主題的某單詞，在該主題中所佔比例\n",
    "    也就是某單詞在該主題中出現的機率　(已導入平滑項)\n",
    "    '''\n",
    "    return ((topic_word_counts[topic][word] + beta) / \n",
    "           (topic_counts[topic] + W * beta))\n",
    "\n",
    "# 建立新函數來計算新的權重值\n",
    "def topic_weight(d, word, k):\n",
    "    '''\n",
    "    給定一個文件，以及文件中的一個單詞，\n",
    "    就可以送回第 k 個主題的權重值\n",
    "    '''\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                       for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 為每個單詞指定某個隨機的主題\n",
    "# 逐一計算出前面所提的相應計數值\n",
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                  for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "        \n",
    "# 以 Gibbs取樣的形式來做\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                             document_topics[d])):\n",
    "            # 從計數值中移除這個單詞 / 主題\n",
    "            # 就不會影響到權重值\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            \n",
    "            # 根據新權重值設定一個新的主題\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            \n",
    "            # 然後再把它加回到計數值之中\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic \t word \t\t count\n",
      "0 \t scikit-learn \t 12\n",
      "0 \t pandas \t 12\n",
      "0 \t Big Data \t 10\n",
      "0 \t HBase \t 6\n",
      "0 \t R \t 6\n",
      "0 \t Java \t 6\n",
      "0 \t C++ \t 6\n",
      "0 \t Haskell \t 6\n",
      "0 \t artificial intelligence \t 6\n",
      "0 \t statsmodels \t 6\n",
      "0 \t statistics \t 5\n",
      "0 \t scipy \t 5\n",
      "0 \t numpy \t 5\n",
      "0 \t mathematics \t 5\n",
      "0 \t deep learning \t 5\n",
      "0 \t regression \t 1\n",
      "0 \t Hadoop \t 1\n",
      "0 \t libsvm \t 1\n",
      "1 \t neural networks \t 12\n",
      "1 \t MongoDB \t 11\n",
      "1 \t deep learning \t 7\n",
      "1 \t HBase \t 6\n",
      "1 \t decision trees \t 6\n",
      "1 \t theory \t 6\n",
      "1 \t Mahout \t 6\n",
      "1 \t databases \t 6\n",
      "1 \t Postgres \t 6\n",
      "1 \t MySQL \t 6\n",
      "1 \t Cassandra \t 1\n",
      "1 \t Python \t 1\n",
      "1 \t numpy \t 1\n",
      "1 \t Big Data \t 1\n",
      "1 \t statistics \t 1\n",
      "2 \t Python \t 17\n",
      "2 \t regression \t 17\n",
      "2 \t R \t 17\n",
      "2 \t Java \t 12\n",
      "2 \t Cassandra \t 11\n",
      "2 \t probability \t 11\n",
      "2 \t machine learning \t 11\n",
      "2 \t statistics \t 11\n",
      "2 \t Postgres \t 6\n",
      "2 \t statsmodels \t 6\n",
      "2 \t C++ \t 6\n",
      "2 \t artificial intelligence \t 6\n",
      "2 \t HBase \t 6\n",
      "2 \t Storm \t 5\n",
      "2 \t programming languages \t 5\n",
      "2 \t MongoDB \t 1\n",
      "2 \t scipy \t 1\n",
      "2 \t mathematics \t 1\n",
      "3 \t Hadoop \t 11\n",
      "3 \t libsvm \t 11\n",
      "3 \t Big Data \t 7\n",
      "3 \t probability \t 7\n",
      "3 \t Spark \t 6\n",
      "3 \t NoSQL \t 6\n",
      "3 \t Python \t 6\n",
      "3 \t MapReduce \t 6\n",
      "3 \t support vector machines \t 6\n",
      "3 \t Storm \t 1\n",
      "3 \t statistics \t 1\n",
      "3 \t machine learning \t 1\n",
      "3 \t programming languages \t 1\n",
      "3 \t R \t 1\n"
     ]
    }
   ],
   "source": [
    "# 找出權重最高的五個單詞\n",
    "print(\"topic \\t\", \"word \\t\\t\", \"count\")\n",
    "\n",
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0: print(k, \"\\t\", word, \"\\t\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "\n",
      "statistics 19 None\n",
      "\n",
      "machine learning 17 None\n",
      "\n",
      "Big Data and programming languages 6 None\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "\n",
      "machine learning 17 None\n",
      "\n",
      "databases 7 None\n",
      "\n",
      "statistics 6 None\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "\n",
      "Big Data and programming languages 22 None\n",
      "\n",
      "machine learning 12 None\n",
      "\n",
      "databases 2 None\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "\n",
      "machine learning 22 None\n",
      "\n",
      "Big Data and programming languages 7 None\n",
      "\n",
      "statistics 1 None\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "\n",
      "machine learning 11 None\n",
      "\n",
      "statistics 7 None\n",
      "\n",
      "databases 6 None\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "\n",
      "machine learning 16 None\n",
      "\n",
      "Big Data and programming languages 13 None\n",
      "\n",
      "statistics 7 None\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "\n",
      "Big Data and programming languages 10 None\n",
      "\n",
      "databases 7 None\n",
      "\n",
      "statistics 6 None\n",
      "\n",
      "machine learning 1 None\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "\n",
      "databases 12 None\n",
      "\n",
      "machine learning 6 None\n",
      "\n",
      "Big Data and programming languages 6 None\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "\n",
      "databases 13 None\n",
      "\n",
      "Big Data and programming languages 10 None\n",
      "\n",
      "statistics 1 None\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "\n",
      "Big Data and programming languages 12 None\n",
      "\n",
      "statistics 11 None\n",
      "\n",
      "machine learning 1 None\n",
      "['statistics', 'R', 'statsmodels']\n",
      "\n",
      "machine learning 12 None\n",
      "\n",
      "Big Data and programming languages 6 None\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "\n",
      "machine learning 17 None\n",
      "\n",
      "Big Data and programming languages 5 None\n",
      "\n",
      "databases 1 None\n",
      "\n",
      "statistics 1 None\n",
      "['pandas', 'R', 'Python']\n",
      "\n",
      "machine learning 11 None\n",
      "\n",
      "Big Data and programming languages 6 None\n",
      "\n",
      "statistics 1 None\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "\n",
      "databases 29 None\n",
      "\n",
      "machine learning 1 None\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "\n",
      "statistics 11 None\n",
      "\n",
      "machine learning 6 None\n",
      "\n",
      "Big Data and programming languages 1 None\n"
     ]
    }
   ],
   "source": [
    "# 替主題指定下列名稱\n",
    "topic_names = [\"Big Data and programming languages\",\n",
    "               \"databases\",\n",
    "               \"machine learning\",\n",
    "               \"statistics\"]\n",
    "\n",
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    for topic, count in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count,\n",
    "                 print())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
